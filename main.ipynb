{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcv7lC2gzyXu",
        "outputId": "fb003db7-c2ed-47b4-e894-e623c116e0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available? True\n",
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 26, 26]              72\n",
            "       BatchNorm2d-2            [-1, 8, 26, 26]              16\n",
            "         LeakyReLU-3            [-1, 8, 26, 26]               0\n",
            "           Dropout-4            [-1, 8, 26, 26]               0\n",
            "            Conv2d-5           [-1, 16, 24, 24]           1,152\n",
            "       BatchNorm2d-6           [-1, 16, 24, 24]              32\n",
            "         LeakyReLU-7           [-1, 16, 24, 24]               0\n",
            "           Dropout-8           [-1, 16, 24, 24]               0\n",
            "            Conv2d-9            [-1, 8, 24, 24]             128\n",
            "        MaxPool2d-10            [-1, 8, 12, 12]               0\n",
            "           Conv2d-11           [-1, 12, 10, 10]             864\n",
            "      BatchNorm2d-12           [-1, 12, 10, 10]              24\n",
            "        LeakyReLU-13           [-1, 12, 10, 10]               0\n",
            "          Dropout-14           [-1, 12, 10, 10]               0\n",
            "           Conv2d-15             [-1, 16, 8, 8]           1,728\n",
            "      BatchNorm2d-16             [-1, 16, 8, 8]              32\n",
            "        LeakyReLU-17             [-1, 16, 8, 8]               0\n",
            "          Dropout-18             [-1, 16, 8, 8]               0\n",
            "           Conv2d-19             [-1, 24, 6, 6]           3,456\n",
            "      BatchNorm2d-20             [-1, 24, 6, 6]              48\n",
            "        LeakyReLU-21             [-1, 24, 6, 6]               0\n",
            "          Dropout-22             [-1, 24, 6, 6]               0\n",
            "AdaptiveAvgPool2d-23             [-1, 24, 1, 1]               0\n",
            "           Conv2d-24             [-1, 10, 1, 1]             240\n",
            "================================================================\n",
            "Total params: 7,792\n",
            "Trainable params: 7,792\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.58\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.62\n",
            "----------------------------------------------------------------\n",
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.03617025539278984 Batch_id=1874 Accuracy=93.24: 100%|██████████| 1875/1875 [00:25<00:00, 73.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0902, Accuracy: 9732/10000 (97.32%)\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.027153944596648216 Batch_id=1874 Accuracy=98.13: 100%|██████████| 1875/1875 [00:22<00:00, 83.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0428, Accuracy: 9868/10000 (98.68%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.020487962290644646 Batch_id=1874 Accuracy=98.49: 100%|██████████| 1875/1875 [00:22<00:00, 83.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0315, Accuracy: 9905/10000 (99.05%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11524567753076553 Batch_id=1874 Accuracy=98.69: 100%|██████████| 1875/1875 [00:23<00:00, 79.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 9899/10000 (98.99%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0017154200468212366 Batch_id=1874 Accuracy=98.88: 100%|██████████| 1875/1875 [00:22<00:00, 81.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0268, Accuracy: 9918/10000 (99.18%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.018137488514184952 Batch_id=1874 Accuracy=99.05: 100%|██████████| 1875/1875 [00:22<00:00, 83.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0271, Accuracy: 9911/10000 (99.11%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.011279946193099022 Batch_id=1874 Accuracy=99.05: 100%|██████████| 1875/1875 [00:21<00:00, 86.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0251, Accuracy: 9928/10000 (99.28%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0017089060274884105 Batch_id=1874 Accuracy=99.14: 100%|██████████| 1875/1875 [00:21<00:00, 85.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0255, Accuracy: 9918/10000 (99.18%)\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0162324458360672 Batch_id=1874 Accuracy=99.16: 100%|██████████| 1875/1875 [00:22<00:00, 85.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0232, Accuracy: 9929/10000 (99.29%)\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.02392140030860901 Batch_id=1874 Accuracy=99.25: 100%|██████████| 1875/1875 [00:21<00:00, 85.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0237, Accuracy: 9932/10000 (99.32%)\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.03928606957197189 Batch_id=1874 Accuracy=99.28: 100%|██████████| 1875/1875 [00:22<00:00, 85.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0216, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.002253919141367078 Batch_id=1874 Accuracy=99.32: 100%|██████████| 1875/1875 [00:22<00:00, 84.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.011167704127728939 Batch_id=1874 Accuracy=99.33: 100%|██████████| 1875/1875 [00:22<00:00, 84.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0194, Accuracy: 9938/10000 (99.38%)\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.01149010006338358 Batch_id=1874 Accuracy=99.33: 100%|██████████| 1875/1875 [00:22<00:00, 83.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0206, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.01901928149163723 Batch_id=1874 Accuracy=99.41: 100%|██████████| 1875/1875 [00:22<00:00, 83.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.02643531560897827 Batch_id=1874 Accuracy=99.41: 100%|██████████| 1875/1875 [00:22<00:00, 83.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0218, Accuracy: 9932/10000 (99.32%)\n",
            "\n",
            "\n",
            "Train Accuracy: 99.40666666666667\n",
            "Test Accuracy: 99.32\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "from sample_data.Model_8 import Net\n",
        "from sample_data.data import get_dataloaders\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(1)\n",
        "\n",
        "# dataloader arguments\n",
        "dataloader_args = dict(shuffle=True, batch_size=32, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64, num_workers=0, pin_memory=False)\n",
        "\n",
        "\n",
        "\n",
        "# Get train dataloader and train data\n",
        "data_dir = './data'\n",
        "train_loader, test_loader, exp_data_train, exp_test,test_loader = get_dataloaders(data_dir, dataloader_args['batch_size'], dataloader_args['num_workers'], dataloader_args['pin_memory'])\n",
        "\n",
        "\n",
        "# Test dataloader (fix the error related to the test dataset)\n",
        "#test_transforms = transforms.Compose([transforms.ToTensor(),])\n",
        "#exp_test = datasets.MNIST(data_dir, train=False, download=True, transform=test_transforms)\n",
        "#test_loader = torch.utils.data.DataLoader(exp_test, **dataloader_args)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "#scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "EPOCHS = 16\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(data)\n",
        "        loss = F.nll_loss(y_pred, target)\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = y_pred.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(desc=f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100 * correct / processed:0.2f}')\n",
        "        train_acc.append(100 * correct / processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.2f}%)\\n')\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    scheduler.step()\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "# Print train and test accuracy\n",
        "print(\"\\nTrain Accuracy:\", train_acc[-1])\n",
        "print(\"Test Accuracy:\", test_acc[-1])\n"
      ]
    }
  ]
}